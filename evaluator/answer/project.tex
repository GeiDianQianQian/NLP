%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage[]{algorithm2e}
\usepackage[english]{babel}
\usepackage[bottom]{footmisc}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{CMPT 413/825 Project\\Using ROSE metric for Machine Translation Evaluation}

\author{Luiz Peres de Oliveira \\
  {\tt lperesde@sfu.ca} \\\And
  Turash Mosharraf \\
  {\tt tmosharr@sfu.ca} \\\And
  Jin Jung \\
  {\tt sjjung@sfu.ca} \\\And
  Justin Lew \\
  {\tt jylew@sfu.ca}
  }

\date{}

\begin{document}
\maketitle

\section{Motivation}

The motivation of using regression and ranking based optimisation in the machine translation evaluation problem is to improve the accuracy of the evaluation beyond the baseline implementation. The baseline implementation used the METEOR metric for finding precision and recall \cite{lavie2007meteor}.

\section{Approach}

The approach taken to improve from the baseline is partially based on the implementation of the ROSE metric \cite{song2011regression}. The algorithm used two feature vectors. The first feature vector consist of the parameters, preceded by it's ID:
	\begin{itemize}
		\item 1-4: n-gram precision, n=1...4
		\item 5-8: n-gram recall, n=1...4
		\item 9-12: n-gram f-measure, n=1...4
		\item 13: average n-gram precision for sentence
		\item 14: score sentence at Document level
	\end{itemize}
The second feature vector consist of the parameters, preceded by it's ID:
	\begin{itemize}
		\item 1-4: n-gram precision for sentence excluding stopwords, n=1...4
		\item 5-8: n-gram recall for sentence excluding stopwords, n=1...4
		\item 9-12 n-gram f-measure for sentence excluding stopwords, n=1...4
		\item 13 average n-gram precision for sentence excluding stopwords, n=1...4
	\end{itemize}

One way to compare the two translations is using n-gram precision and n-gram recall. n-gram precision is the ratio of the count of n-grams in the candidate translation sentence that is in the reference sentence to the counts of all n-grams in the candidate sentence. The n-gram precision is defined as 
	\[P_{n} = \frac{\sum_{ngram \in \vec{c}, ngram \in \vec{r}} Count(ngram)}{\sum_{ngram\in \vec{c}} Count(ngram)  } \]

where $\vec{r}$, is defined as the (human) reference sentence, and $\vec{c}$, is defined as the (hypothesis) candidate sentence. Only 1,2,3,4-gram counts are used in the implementation of the algorithm.

\section{Data}

The data file to train the evaluation model is from hyp1-hyp2-ref. The file consists of a triple (hyp1, hyp2, and ref) where hyp1 and hyp2 are two translations to which is evaluated by the algorithm, along with a reference sentence of the hypothesis curated by a human translator.

The data file dev.answers contains the preference between the two hypothesis translations by a human translator. The numbers correspond to outputs of the function,
	\[ f(h_{1}, h_{2}, e) = 
		\begin{cases}
			\text{1, } &\quad\text{ if $h_{1}$ is preferred to $h_{2}$}\\
			\text{0, } &\quad\text{if $h_{1}$ is equally good/bad to $h_{2}$}\\
			\text{-1, } &\quad\text{if $h_{2}$ is preferred to $h_{1}$ }
		\end{cases} \]
where $h_{1}$ and $h_{2}$ are the two hypothesis translation and $e$ is the reference translation.

\section{Code}

\subsection{Pseudocode of modified ROSE metric evaluation}
\begin{algorithm}
	\KwData{$(hyp1,hyp2,ref)$ in $\mathcal{D} $}
	\KwResult{ output the function for each triple (hyp1, hyp2, ref) in $\mathcal{D}$}
	\For{ $(h1, h2, e)$ in $\mathcal{D}$}{
		vc1, vc2 = [0] * 32, [0] * 32;\\
		sw1, sw2 = [0] * 13, [0] * 13;\\
		h1 = $fix\_input$(h1);\\
		h2 = $fix\_input$(h2);\\
		(vc1, vc2) = $get\_ngrams$(e, h1, h2, vc1, vc2, TRUE);\\
		(sw1, sw2) = $get\_ngrams$(rsw(e), rsw(h1), rsw(h2), sw1, sw2, FALSE);\\
		l1 = ($sum$(vc1[0:13]) + $sum$(sw1) * 1.1) + (vc1[13] * 0.4 ))/2.5;\\
		l2 = ($sum$(vc2[0:13]) + $sum$(sw2) * 1.1) + (vc1[13] * 0.4 ))/2.5;\\
		\uIf{l1 == l2}{
			print 0\;}
		\uElseIf{$l1 < l2$}{
			print 1\;}
		\Else{
			print -1\;
			}
	}
\end{algorithm}

\subsection{Miscellaneous algorithms used}

The function $get\_ngrams(\cdot)$ is used to calculate the ratio of the n-gram counts $P_{n}$ defined previously. The function $rsw(\cdot)$ outputs the sentence with all the stopwords removed. Stopwords are high frequency words in a given grammar and may be the set that contains the words: "a", "I", and etc.


\section{Experimental Setup}

Our experiment compares the accuracy between ROSE and the METEOR metric as shown in the baseline. Method 1 was the baseline implementation of the METRO metric. Methods 2 through 8 are modifications to the ROSE implementation  \cite{song2011regression} and the modifications to the sentence structure of the data set. 
Table 1 shows the methods implemented to improve the accuracy of the evaluator.
\begin{center}
	\begin{tabular}{| l | p{7cm} |}
	\hline
	Method & Description \\ \hline
	1 & METEOR \\ \hline
	2 & ROSE, only one feature vector with 13 elements \\ \hline
	3 & ROSE, added second feature vector with sentences without stopwords \\ \hline
	4 & Removed all characters with punctuation \\ \hline
	5 & Included scores for n-grams at sentence level. First feature vector contains 14 elements.  \\ \hline
	6 & Removed all unicode characters and used WordNet to lemmatize sentence input. \\ \hline
	7 & Used WordNet to check similarities among words \\ \hline
	8 & Used levenshtein distance \\ \hline
	\end{tabular}
\end{center}
	

\subsection{Results}

\begin{center}
	\begin{tabular}{| l | l | l | l |}
	\hline
	Method & Time Execution $\footnotemark[1] $& Dev Score & Test Score \\ \hline
	1 & 1min 33sec & 0.510169 & 0.529 \\ \hline
	2 & 11sec & 0.512868 & 0.529 \\ \hline
	3 & 50sec & 0.517365 & 0.533 \\ \hline
	4 & 48sec & 0.519008 & 0.539 \\ \hline
	5 & 55sec & 0.520103 & 0.541 \\ \hline
	6 & 4min 51sec & 0.523115 & 0.546 \\ \hline
	7 & 5min 43sec & 0.526166 & 0.547 \\ \hline
	8 & 6min 17sec & 0.530742 & 0.548 \\ \hline
	\end{tabular}
\end{center}
\footnotetext[1]{Time of execution is based on the performance of a 2.2GHz quad-core Intel Core i7 processor MacBook Pro}

\subsection{Analysis of the Results}

\section{Future Work}
Future work that extends the partial implementation of the ROSE metric would be to use a special kernel for categorizing text documents \cite{lodhi2002text}. In order to improve upon ROSE and BLEU, methods for combining scores from partial syntactic dependency matches along with n-gram matches using a statistical parser as presented in the paper \cite{kahn2009expected}.

\bibliographystyle{acl2012}
\bibliography{References}

\end{document}



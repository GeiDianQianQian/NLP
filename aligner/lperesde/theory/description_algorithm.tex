%=======================CIS 9615 LaTeX template==================
%
% How to use:
%    1. Update your information in section "A" below
%    2. Write your answers in section "B" below. Precede answers for all 
%       parts of a question with the command "\question{n}{desc}" where n is
%       the question number and "desc" is a short, one-line description of 
%       the problem. There is no need to restate the problem.
%    3. If a question has multiple parts, precede the answer to part x with the
%       command "\part{x}".
%    4. If a problem asks you to design an algorithm, use the commands
%       \algorithm, \correctness, \runtime to precede your discussion of the 
%       description of the algorithm, its correctness, and its running time, respectively.
%    5. You can include graphics by using the command \includegraphics{FILENAME}
%
\documentclass[11pt]{article}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{url}
\usepackage[noend]{algorithmic}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{color}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{textcomp}
\setlength{\arrayrulewidth}{1pt}

\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{lemma}{\textbf{Lemma}}
\newtheorem{proposition}{\textbf{Proposition}}
\newtheorem{definition}{\textbf{Definition}}
\newtheorem{corollary}{\textbf{Corollary}}
\newtheorem{conjecture}{\textbf{Conjecture}}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{url}
\usepackage[noend]{algorithmic}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
%\newcommand\algorithm{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\prof{\vspace{.10in}\textbf{Proof: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\base {\vspace{.10in}\textbf{Base Case: }}
\newcommand\inductive {\vspace{.10in}\textbf{Inductive Step: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\pagestyle{fancyplain}
\lhead{\textbf{\NAME}}
\lhead{\textbf{\NAME\ (\ID)}}
\chead{\textbf{\HWNUM}}
\rhead{CMPT 825 and CMPT 413, Fall 2017}
%\begin{document}\raggedright
\begin{document}
%Section A==============Change the values below to match your information==================
\newcommand\NAME{Turash and Luiz}  % your name
\newcommand\ID{tmosharr \& lperesde}     % your Temple AccessNet Account
\newcommand\HWNUM{}              % the homework number



%Section B==============Put your answers to the questions below here=======================

% no need to restate the problem --- the graders know which problem is which,
% but replacing "The First Problem" with a short phrase will help you remember
% which problem this is when you read over your homeworks to study.
%send to : HW3Subm.43vjts2a9ie04m7x@u.box.com
\question{A}{Theory} 

The algorithm used on this assignment was an enhanced version of the baseline algorithm. The enhancements are listed below:

\begin{itemize}


\item  Considered both $p(f|e)$ and $p(e|f)$. The output is the intersection of both sets.
\item  Added NULL words to the end of each $f$ and $e$ sentences.
\item  The alignments considered are the nearest to the current index $i$. According to the last sentence on \textit{pg. 837} of the HMM-Based Word Alignment in Statistical Translation paper (http://aclweb.org/anthology/C/C96/C96-2141.pdf), the difference in the position index is smaller than 3 (for Indoeuropean languages). 
\item  Applied probability smoothing for both $p(f|e)$ and $p(e|f)$.
Assuming we have a source vocabulary of size $v_f$ and a target vocabulary of size  $v_e$, the formula for calculating probabilities are smoothed as the followings:

\begin{equation}
P(f|e) = \frac{(count_{f,e} + \delta)}{(count_{e} + \delta * v_f)}
\end{equation} 
\begin{equation}
P(e|f) = \frac{(count_{e,f} + \delta)}{(count_{f} + \delta * v_e)}
\end{equation} 

Here, $\delta$ is a very small constant. We chose it to be 0.01

\end{itemize}
\question{B}{Algorithm} 

The algorithms for the both the word alignment and decoding are shown in the next page:	

\begin{algorithm}[htb]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}\hspace{10.7pt}}
	\renewcommand\algorithmicensure {\textbf{Output:} }
	\caption{Espectation Maximization Training}
	\label{alg:pbg}
		\begin{algorithmic}[1]
		\vspace{3pt}
		\REQUIRE
		Set of sentences $F$ from source language and
		Set of sentences $E$ from target language\\
		\ENSURE 
		Probability distributions $P(f|e)$ and $P(e|f)$ 
		for any source word $e$ and target word $f$\\
		\STATE add a last word to represent \textit{NULL} on each $f$ and $e$ sentences.
		\STATE $v_f \gets$ number of distinct source words and $v_e \gets$ number of distinct target words
		\STATE Initialize all counts to 0
		\STATE Initialize $t1_{f|e}\gets \frac{1}{v_f}$ and $t2_{e|f}\gets \frac{1}{v_e}$
		for all possible pair $(f,e)$
		\STATE Initialize $\delta \gets$ 0.01  
		\WHILE{$Epoch \le MaxEpoch$}
		  \STATE $Epoch \gets Epoch+1$
		  \FOR {each pair of sentences $(f,e)$}
			\FOR {each $f_i$ in $f$}
			  \STATE $c \gets 0$ and $z \gets 0$
			  \FOR {each $e_j$ in $e$}
			    \STATE  $z \gets z + t1_{f_i|e_j}$ 
			  \ENDFOR
			  \FOR {each $e_j$ in $e$}
			    \STATE $count_{f_i,e_j} \gets count_{f_i,e_j}+\frac{t1_{f_i|e_j}}{z}$ and $count_{e_j} \gets count_{e_j}+\frac{t1_{f_i|e_j}}{z}$ 
			  \ENDFOR
			\ENDFOR
			\FOR {each $e_i$ in $e$}
			  \STATE $c \gets 0$ and $z \gets 0$
			  \FOR {each $f_j$ in $f$}
			     \STATE  $z \gets z + t2_{e_i|f_j}$
			  \ENDFOR
			  \FOR {each $f_j$ in $f$}
			    \STATE $count_{e_i,f_j} \gets count_{e_i,f_j}+\frac{t2_{e_i|f_j}}{z}$ and $count_{f_j} \gets count_{f_j}+\frac{t2_{e_i|f_j}}{z}$
			  \ENDFOR
			\ENDFOR
		  \ENDFOR
		  
		  \FOR {each pair $(f,e)$ in $count_{f,e}$}
		    \STATE $t1_{f|e} \gets \frac{(count_{f,e} + \delta)}{(count_{e} + \delta * v_f)}$
		  \ENDFOR
		  
		  \FOR {each pair $(e,f)$ in $count_{e,f}$}
		    \STATE $t2_{e|f} \gets \frac{(count_{e,f} + \delta)}{(count_{f} + \delta * v_e)}$
		  \ENDFOR
		\ENDWHILE 
	  \RETURN $t1_{f|e}$ and $t2_{e|f}$ for all $e$ and $f$
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}\hspace{10.7pt}}
	\renewcommand\algorithmicensure {\textbf{Output:} }
	\caption{Decoding}
	\label{alg:pbg}
		\begin{algorithmic}[1]
		\vspace{3pt}
		\REQUIRE
		Probability distributions $t1_{f|e}$ and $t2_{e|f}$ 
		for any source word $e$ and target word $f$\\
		\ENSURE Set of aligned word pairs $(f_i,e_j)$\\
		\STATE $AlignedWords \gets \emptyset$
		\FOR {each pair of sentences $(f,e)$}
			\STATE Ignore possible $NULL$ alignments
			\STATE $list_{FE} \gets \emptyset$ and $list_{EF} \gets \emptyset$
			\FOR {each $f_i$ in $f$}
			 	\STATE $best_j \gets argmax_{e_j} t1_{f_i|e_j}$ \textit{// if $p(j) = p(best_j)$ the nearest distance $d(f_aj, e_j)$ is chosen}
			  	\STATE add pair $(f_i,e_j)$ to $list_{FE}$
			\ENDFOR
			\FOR {each $e_i$ in $e$}
			 	\STATE $best_j \gets argmax_{f_j} t2_{e_i|f_j}$ \textit{// if $p(j) = p(best_j)$ the nearest distance $d(e_aj, f_j)$ is chosen}
			  	\STATE add pair $(f_j,e_i)$ to $list_{EF}$
			\ENDFOR
			\STATE add $list_{FE} \cup list_{EF}$ to $AlignedWords$
		\ENDFOR
	    \RETURN $AlignedWords$ 
	\end{algorithmic}
\end{algorithm}

\end{document}